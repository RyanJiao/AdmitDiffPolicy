<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Admittance Visuomotor Policy for Contact-Rich
        Manipulation Tasks with Hand-Arm Teleoperation.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Admittance Visuomotor Policy for Contact-Rich Manipulation Tasks with Hand-Arm Teleoperation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- 题目， 作者， 链接 部分 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Admittance Visuomotor Control Policy Learning for Contact-Rich Manipulation Through Force-Interactive Teleoperation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Bo Zhou</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Ruixuan Jiao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Yi Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Xiaogang Yuan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Fang Fang</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Southeast University,</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- arixv link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 正文部分 从视频网格到图片 -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item open_left_door_mute">
          <video poster="" id="open_left_door_mute" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/Insertion.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item open_cabinet_drawer_mute">
          <video poster="" id="open_cabinet_drawer_mute" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/Wipe.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item open_right_door_mute">
          <video poster="" id="open_right_door_mute" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/Drag.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item open_top_drawer_mute">
          <video poster="" id="open_top_drawer_mute" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/Open.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item open_washing_machine_mute">
          <video poster="" id="open_washing_machine_mute" autoplay controls muted loop playsinline height="100%">
            <source src="static/videos/Draw.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
  

<!-- 大图
<section class="section">
  <div class="column is-six-fifths">
    <img id="framework" width="100%" src="static/images/general_fig.png">
  </div>
</section> -->

<!-- 摘要文字 -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Operating smoothly in contact-rich environments is crucial for robots to effectively perform 
            daily tasks. Leveraging contact force information can enhance the smoothness of interactive 
            operations, force control should be addressed throughout the entire process of autonomous 
            policy development, from data collection, training to deployment.
          </p>
          <p>
            we propose an admittance imitation learning visuomotor policy framework to reduce mean contact 
            force and force fluctuations. Our framework utilizes RGB images, robot joint positions, end-effector 
            poses, and contact force as inputs, employing a diffusion model to generate future end-effector 
            trajectories and contact force.
          </p>
          <p>
            An admittance controller is employed
            to track this trajectories, enabling effective force control
            for various tasks. Furthermore, a low-cost hand-arm tele-
            operation system with interactive feedback is designed for
            data collection. A evaluation of our teleoperation system
            and policy framework was conducted on five contact-rich
            manipulation tasks, each representing an action primitive.
            Results show that our framework achieves the highest suc-
            cess rate and demonstrates smoother contact compared
            to other methods. Particularly for door opening task, the
            average contact force is reduced by 53.92%, while the stan-
            dard deviation of contact force fluctuations is diminished
            by 76.51%.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <!-- youtube视频 -->
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


  <!-- 整体方法 -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method -->
        <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">AdmitDiff Policy Framework</h2>

          <h3 class="has-text-centered">
            <div class="content has-text-justified">
            <p>The observation images of the expert data are converted to representation vectors by a visual encoder. In
              the rollout, the observation image is converted to a representation vector with the same visual encoder.
              The action and target wrench are predicted by retrieving the action and wrench of the expert data with
              top-1 similarity of the representations. We use admittance whole-body control (a-WBC) to control the robot.
            </p>
          </div>
          </h3>

          <img id="framework" width="90%" src="static/images/policy_all.png">

        </div>
      </div>
    </div>
  </section>

  <!-- 遥操作 -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method -->
        <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Teleoperation</h2>

          <h3 class="has-text-centered">
            <div class="content has-text-justified">
            <p>The observation images of the expert data are converted to representation vectors by a visual encoder. In
              the rollout, the observation image is converted to a representation vector with the same visual encoder.
              The action and target wrench are predicted by retrieving the action and wrench of the expert data with
              top-1 similarity of the representations. We use admittance whole-body control (a-WBC) to control the robot.
            </p>
          </div>
          </h3>     

          <img id="framework" width="60%" src="static/images/teleop.png">
          
        </div>
      </div>
    </div>
  </section>

  <!-- open_left_door -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Behavior Cloning -->
      <div class="column">
        <div class="content">
          <p>Open Box</p>
          <video id="open_left_door_bc" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tele_openbox.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA -->
      <div class="column">
        <div class="content">
          <p>Rotate Box</p>
          <video id="open_left_door_moma" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tele_rotatbox.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA-Force -->
      <div class="column">
        <div class="content">
          <p>Drag</p>
          <video id="open_left_door_moma_force" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tele_drag.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>


    </div>
  </div>


<!-- 结果 -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
      <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results</h2>
        <img id="framework" width="100%" src="static/images/policy_all.png">
        <h3 class="has-text-centered">
          <div class="content has-text-justified">
          <p>The observation images of the expert data are converted to representation vectors by a visual encoder. In
            the rollout, the observation image is converted to a representation vector with the same visual encoder.
            The action and target wrench are predicted by retrieving the action and wrench of the expert data with
            top-1 similarity of the representations. We use admittance whole-body control (a-WBC) to control the robot.
          </p>
        </div>
        </h3>
      </div>
    </div>
  </div>
</section>


<!-- 九宫格视频 -->
<section class="section">

  <!-- open_right_door -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Behavior Cloning -->
      <div class="column">
        <div class="content">
          <p>Behavior Cloning (BC)</p>
          <video id="BC" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_right_door_bc_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA -->
      <div class="column">
        <div class="content">
          <p>MOMA-Force w/o FC</p>
          <video id="MOMA" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_right_door_moma_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA-Force -->
      <div class="column">
        <div class="content">
          <p>MOMA-Force</p>
          <video id="MOMA-Force" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_right_door_moma_force_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>


    </div>
  </div>
  <!-- open_right_door end -->

  <!-- open_left_door -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Behavior Cloning -->
      <div class="column">
        <div class="content">
          <video id="open_left_door_bc" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_left_door_bc_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA -->
      <div class="column">
        <div class="content">
          <video id="open_left_door_moma" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_left_door_moma_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA-Force -->
      <div class="column">
        <div class="content">
          <video id="open_left_door_moma_force" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/open_left_door_moma_force_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>


    </div>
  </div>
  <!-- open_left_door end -->

  <!-- rotate_tap -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Behavior Cloning -->
      <div class="column">
        <div class="content">
          <video id="rotate_tap_bc" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/rotate_tap_bc_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA -->
      <div class="column">
        <div class="content">
          <video id="rotate_tap_moma" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/rotate_tap_moma_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <!-- MOMA-Force -->
      <div class="column">
        <div class="content">
          <video id="rotate_tap_moma_force" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/comp/rotate_tap_moma_force_720.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>


    </div>
  </div>
  <!-- open_left_door end -->
</section>

<!-- 两个视频 -->
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>


</body>
</html>
